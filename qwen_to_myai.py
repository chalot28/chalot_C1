#!/usr/bin/env python3
"""
=============================================================================
qwen_to_myai.py ‚Äî Chuy·ªÉn ƒë·ªïi Qwen2.5-0.5B sang ƒë·ªãnh d·∫°ng .myai
=============================================================================

Quy tr√¨nh:
1. T·∫£i Qwen2.5-0.5B-Instruct t·ª´ HuggingFace
2. Up-cycle th√†nh MoE (nh√¢n b·∫£n FFN ‚Üí 8 experts v·ªõi nhi·ªÖu Gaussian)
3. Quantize:
   - Attention weights ‚Üí Int8
   - Expert weights ‚Üí Int4
4. Ph√¢n v√πng Brain Map (Shallow/Deep/Fact)
5. Xu·∫•t ra file .myai

Y√™u c·∫ßu:
    pip install torch safetensors transformers numpy
    huggingface-cli login  # N·∫øu c·∫ßn

Ch·∫°y:
    python qwen_to_myai.py --model Qwen/Qwen2.5-0.5B-Instruct --output qwen_moe.myai
"""

import argparse
import struct
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple
import torch
from safetensors import safe_open
from transformers import AutoConfig


# =============================================================================
# C·∫•u h√¨nh Qwen2.5-0.5B
# =============================================================================
QWEN_CONFIG = {
    'dim': 896,
    'hidden_dim': 4864,
    'n_layers': 24,
    'n_heads': 14,
    'vocab_size': 151936,
    'max_seq_len': 2048,
    'n_experts': 8,
    'top_k': 2,
    'int4_group_size': 32,
}

# Brain Map layer ranges
BRAIN_MAP_RANGES = {
    'shallow': (0, 6),    # Layers 0-5
    'deep': (6, 18),      # Layers 6-17
    'fact': (18, 24),     # Layers 18-23
}

EXPERT_NOISE_STD = 0.01


# =============================================================================
# Quantization Functions
# =============================================================================
def quantize_int8(weights: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Quantize Float32 ‚Üí Int8 v·ªõi per-tensor scaling
    Returns: (int8_weights, scale_factor)
    """
    max_val = np.abs(weights).max()
    scale = max_val / 127.0 if max_val > 0 else 1.0
    
    quantized = np.clip(weights / scale, -128, 127).astype(np.int8)
    return quantized, np.array([scale], dtype=np.float32)


def quantize_int4(weights: np.ndarray, group_size: int = 32) -> Tuple[np.ndarray, np.ndarray]:
    """
    Quantize Float32 ‚Üí Int4 v·ªõi group-wise scaling
    Returns: (packed_int4_bytes, scales_per_group)
    
    Format: 2 gi√° tr·ªã Int4 ƒë√≥ng g√≥i trong 1 byte
    """
    flat = weights.flatten()
    n_groups = (len(flat) + group_size - 1) // group_size
    scales = np.zeros(n_groups, dtype=np.float32)
    
    # Quantize t·ª´ng group
    quantized = np.zeros(len(flat), dtype=np.int8)
    for i in range(n_groups):
        start = i * group_size
        end = min(start + group_size, len(flat))
        group = flat[start:end]
        
        max_val = np.abs(group).max()
        scale = max_val / 7.0 if max_val > 0 else 1.0
        scales[i] = scale
        
        quantized[start:end] = np.clip(group / scale, -8, 7).astype(np.int8)
    
    # Pack 2 Int4 values v√†o 1 byte
    packed = []
    for i in range(0, len(quantized), 2):
        if i + 1 < len(quantized):
            # ƒê√≥ng g√≥i: (val1 & 0xF) | ((val2 & 0xF) << 4)
            val1 = quantized[i] & 0xF
            val2 = quantized[i + 1] & 0xF
            packed.append((val1 | (val2 << 4)) & 0xFF)
        else:
            packed.append(quantized[i] & 0xF)
    
    return np.array(packed, dtype=np.uint8), scales


# =============================================================================
# MoE Up-cycling
# =============================================================================
def upcycle_ffn_to_moe(
    gate_proj: np.ndarray,
    up_proj: np.ndarray,
    down_proj: np.ndarray,
    n_experts: int = 8,
    noise_std: float = 0.01
) -> List[Dict[str, np.ndarray]]:
    """
    Nh√¢n b·∫£n FFN g·ªëc th√†nh n_experts experts v·ªõi nhi·ªÖu Gaussian
    
    Returns: List[{'gate_proj', 'up_proj', 'down_proj'}]
    """
    experts = []
    for i in range(n_experts):
        # Th√™m nhi·ªÖu ƒë·ªÉ t·∫°o s·ª± ƒëa d·∫°ng
        noise_gate = np.random.normal(0, noise_std, gate_proj.shape).astype(np.float32)
        noise_up = np.random.normal(0, noise_std, up_proj.shape).astype(np.float32)
        noise_down = np.random.normal(0, noise_std, down_proj.shape).astype(np.float32)
        
        experts.append({
            'gate_proj': gate_proj + noise_gate,
            'up_proj': up_proj + noise_up,
            'down_proj': down_proj + noise_down,
        })
    
    return experts


def create_router_weights(dim: int, n_experts: int) -> np.ndarray:
    """
    T·∫°o router weights ng·∫´u nhi√™n cho MoE gating
    Shape: [dim, n_experts]
    """
    # Xavier initialization
    limit = np.sqrt(6.0 / (dim + n_experts))
    return np.random.uniform(-limit, limit, (dim, n_experts)).astype(np.float32)


# =============================================================================
# ƒê·ªçc Safetensors t·ª´ HuggingFace
# =============================================================================
def load_qwen_weights(model_path: str) -> Dict[str, np.ndarray]:
    """
    Load weights t·ª´ Qwen2.5-0.5B safetensors files
    Returns: Dict[layer_name -> numpy array]
    """
    model_dir = Path(model_path)
    weights = {}
    
    # T√¨m t·∫•t c·∫£ c√°c file .safetensors
    safetensor_files = list(model_dir.glob("*.safetensors"))
    if not safetensor_files:
        # Th·ª≠ t·∫£i t·ª´ cache HuggingFace
        from huggingface_hub import snapshot_download
        model_dir = Path(snapshot_download(repo_id=model_path))
        safetensor_files = list(model_dir.glob("*.safetensors"))
    
    print(f"[load] T√¨m th·∫•y {len(safetensor_files)} file safetensors")
    
    for st_file in safetensor_files:
        print(f"[load] ƒê·ªçc {st_file.name}...")
        with safe_open(st_file, framework="numpy") as f:
            for key in f.keys():
                weights[key] = f.get_tensor(key)
    
    print(f"[load] ƒê√£ load {len(weights)} tensors")
    return weights


# =============================================================================
# Chuy·ªÉn ƒë·ªïi ch√≠nh
# =============================================================================
def convert_qwen_to_myai(model_path: str, output_path: str):
    """
    Pipeline ch√≠nh: Qwen ‚Üí MoE ‚Üí Quantized ‚Üí .myai
    """
    print("=" * 80)
    print("QWEN2.5-0.5B ‚Üí AI_CHALOT_C1 MoE CONVERTER")
    print("=" * 80)
    
    # 1. Load weights
    print("\n[1/5] Loading Qwen2.5-0.5B weights...")
    weights = load_qwen_weights(model_path)
    
    # 2. Chu·∫©n b·ªã c·∫•u tr√∫c
    print("\n[2/5] Preparing architecture...")
    config = QWEN_CONFIG
    
    # T·∫°o file .myai
    with open(output_path, 'wb') as f:
        # Write header (256 bytes)
        write_header(f, config)
        
        # 3. Write embedding
        print("\n[3/5] Processing embeddings...")
        write_embeddings(f, weights, config)
        
        # 4. Write layers v·ªõi MoE up-cycling
        print("\n[4/5] Converting layers with MoE up-cycling...")
        write_layers_moe(f, weights, config)
        
        # 5. Write output head
        print("\n[5/5] Writing output layer...")
        write_output(f, weights, config)
    
    print(f"\n‚úÖ Conversion complete: {output_path}")
    print(f"   File size: {Path(output_path).stat().st_size / 1e6:.1f} MB")


def write_header(f, config: dict):
    """
    Ghi header 256 bytes theo format model/header.rs
    """
    # Magic "MYAI" (4 bytes)
    f.write(struct.pack('<I', 0x4D594149))
    
    # Version 2 (4 bytes)
    f.write(struct.pack('<I', 2))
    
    # Core config (8 fields √ó 4 bytes = 32 bytes)
    f.write(struct.pack('<I', config['dim']))
    f.write(struct.pack('<I', config['hidden_dim']))
    f.write(struct.pack('<I', config['n_layers']))
    f.write(struct.pack('<I', config['n_heads']))
    f.write(struct.pack('<I', config['vocab_size']))
    
    # Flags: bit0=quantized, bit1=int4_experts, bit2=moe
    flags = 0b111  # Quantized + Int4 + MoE
    f.write(struct.pack('<I', flags))
    
    # Padding ƒë·∫øn offset 128
    f.write(b'\x00' * (128 - 8 - 24))
    
    # V2 fields (offset 128)
    f.write(struct.pack('<I', config['n_experts']))
    f.write(struct.pack('<I', config['top_k']))
    f.write(struct.pack('<I', config['int4_group_size']))
    f.write(struct.pack('<I', 8))  # depth_router_layer
    f.write(struct.pack('<I', config['max_seq_len']))
    
    # Padding ƒë·∫øn 256 bytes
    f.write(b'\x00' * (256 - 128 - 20))
    
    print(f"[header] Written: v2, dim={config['dim']}, layers={config['n_layers']}, experts={config['n_experts']}")


def write_embeddings(f, weights: dict, config: dict):
    """
    Ghi embedding table (vocab_size √ó dim)
    Quantize Int8 ƒë·ªÉ ti·∫øt ki·ªám
    """
    # T√¨m weight key cho embedding
    embed_key = 'model.embed_tokens.weight'
    if embed_key not in weights:
        raise KeyError(f"Cannot find {embed_key} in weights")
    
    embed = weights[embed_key].astype(np.float32)
    print(f"[embed] Shape: {embed.shape}")
    
    # Quantize Int8
    quantized, scale = quantize_int8(embed)
    
    # Write: [scale (4 bytes)] + [data (vocab_size √ó dim bytes)]
    f.write(scale.tobytes())
    f.write(quantized.tobytes())
    
    print(f"[embed] Written {quantized.nbytes / 1e6:.1f} MB (Int8)")


def write_layers_moe(f, weights: dict, config: dict):
    """
    Ghi 24 layers v·ªõi MoE up-cycling
    M·ªói layer:
      - Attention (QKV, O) ‚Üí Int8
      - LayerNorms ‚Üí Float32
      - Router ‚Üí Float32
      - 8 Experts (gate/up/down) ‚Üí Int4
    """
    dim = config['dim']
    hidden_dim = config['hidden_dim']
    n_experts = config['n_experts']
    group_size = config['int4_group_size']
    
    for layer_idx in range(config['n_layers']):
        print(f"\n[layer {layer_idx:02d}] Processing...")
        prefix = f'model.layers.{layer_idx}'
        
        # 1. Attention weights (Int8)
        q_proj = weights[f'{prefix}.self_attn.q_proj.weight'].astype(np.float32)
        k_proj = weights[f'{prefix}.self_attn.k_proj.weight'].astype(np.float32)
        v_proj = weights[f'{prefix}.self_attn.v_proj.weight'].astype(np.float32)
        o_proj = weights[f'{prefix}.self_attn.o_proj.weight'].astype(np.float32)
        
        for name, w in [('Q', q_proj), ('K', k_proj), ('V', v_proj), ('O', o_proj)]:
            quant, scale = quantize_int8(w)
            f.write(scale.tobytes())
            f.write(quant.tobytes())
        
        # 2. LayerNorms (Float32, nh·ªè n√™n kh√¥ng quantize)
        attn_norm = weights[f'{prefix}.input_layernorm.weight'].astype(np.float32)
        ffn_norm = weights[f'{prefix}.post_attention_layernorm.weight'].astype(np.float32)
        f.write(attn_norm.tobytes())
        f.write(ffn_norm.tobytes())
        
        # 3. Router weights (Float32)
        router = create_router_weights(dim, n_experts)
        f.write(router.tobytes())
        
        # 4. FFN ‚Üí MoE up-cycling (Int4)
        gate_proj = weights[f'{prefix}.mlp.gate_proj.weight'].astype(np.float32)
        up_proj = weights[f'{prefix}.mlp.up_proj.weight'].astype(np.float32)
        down_proj = weights[f'{prefix}.mlp.down_proj.weight'].astype(np.float32)
        
        experts = upcycle_ffn_to_moe(gate_proj, up_proj, down_proj, n_experts, EXPERT_NOISE_STD)
        
        for expert_idx, expert in enumerate(experts):
            for proj_name in ['gate_proj', 'up_proj', 'down_proj']:
                proj = expert[proj_name]
                quant, scales = quantize_int4(proj, group_size)
                
                # Write: [n_scales (4)] + [scales (n_scales √ó 4)] + [data]
                f.write(struct.pack('<I', len(scales)))
                f.write(scales.tobytes())
                f.write(quant.tobytes())
        
        print(f"[layer {layer_idx:02d}] ‚úì Written (Attn: Int8, {n_experts} Experts: Int4)")


def write_output(f, weights: dict, config: dict):
    """
    Ghi output projection (lm_head)
    """
    # Final norm
    norm = weights['model.norm.weight'].astype(np.float32)
    f.write(norm.tobytes())
    
    # LM head (vocab_size √ó dim)
    lm_head = weights['lm_head.weight'].astype(np.float32)
    
    # Quantize Int8
    quant, scale = quantize_int8(lm_head)
    f.write(scale.tobytes())
    f.write(quant.tobytes())
    
    print(f"[output] Written lm_head {quant.nbytes / 1e6:.1f} MB (Int8)")


# =============================================================================
# Main
# =============================================================================
def main():
    parser = argparse.ArgumentParser(description='Convert Qwen2.5-0.5B to .myai format')
    parser.add_argument('--model', type=str, default='Qwen/Qwen2.5-0.5B-Instruct',
                        help='Model path ho·∫∑c HuggingFace repo ID')
    parser.add_argument('--output', type=str, default='qwen_moe.myai',
                        help='Output .myai file path')
    
    args = parser.parse_args()
    
    convert_qwen_to_myai(args.model, args.output)
    
    print("\n" + "=" * 80)
    print("üéâ NEXT STEPS:")
    print("=" * 80)
    print("1. Copy qwen_moe.myai to your Pixel 5:")
    print("   adb push qwen_moe.myai /sdcard/Download/")
    print("")
    print("2. Build & run AI_chalot_C1:")
    print("   cargo build --release --target aarch64-linux-android")
    print("   adb push target/aarch64-linux-android/release/AI_chalot_C1 /data/local/tmp/")
    print("   adb shell chmod +x /data/local/tmp/AI_chalot_C1")
    print("   adb shell /data/local/tmp/AI_chalot_C1 /sdcard/Download/qwen_moe.myai")
    print("")
    print("3. Expect performance:")
    print("   - RAM usage: ~250MB (Top-2 MoE activation)")
    print("   - Speed: 20-25 tokens/sec on Pixel 5")
    print("   - Quality: Comparable to Qwen2.5-0.5B-Instruct")


if __name__ == '__main__':
    main()
